{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "Adapted from [Training with PyTorch](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as tv_transfroms\n",
    "\n",
    "from models.fcn_factory import FcnFactory\n",
    "from preprocess.datasets import make_hou_dataset, make_kasmi_ign_dataset\n",
    "from preprocess import transforms as custom_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 11.8\n",
      "CUDA available: True\n",
      "Device ID: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'CUDA version: {torch.version.cuda}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "print(f'Device ID: {torch.cuda.current_device()}')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "RANDOM_STATE = 42\n",
    "rng = torch.Generator().manual_seed(RANDOM_STATE)\n",
    "N_BATCHES_FOR_RECORD = 10\n",
    "\n",
    "# Warm Start (set to `None` to train head from scratch)\n",
    "WARM_START_MODEL_PATH = None # 'saved_models/model_20240324_152229_4'\n",
    "\n",
    "# Save Test Dataset (if test_ds is set elsewhere then it will be saved)\n",
    "test_ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs\n",
    "EPOCHS = 5\n",
    "\n",
    "# Data loading\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model & Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = FcnFactory(n_classes=2)\n",
    "model = factory.make_fcn('resnet50').to(device)\n",
    "if WARM_START_MODEL_PATH:\n",
    "    model.load_state_dict(torch.load(WARM_START_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_shape(model, in_transform) -> list:\n",
    "    dummy_input = torch.randn((1, 3, 256, 256)).to(device)\n",
    "    return list(model(in_transform(dummy_input))['out'].shape[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_transforms = tv_transfroms.Compose(\n",
    "    [\n",
    "        tv_transfroms.Lambda(custom_transforms.rgba_to_rgb),\n",
    "        factory.input_transforms,\n",
    "    ]\n",
    ")\n",
    "output_h_w = get_output_shape(model, input_transforms)\n",
    "mask_transforms = tv_transfroms.Compose(\n",
    "    [\n",
    "        tv_transfroms.Resize(output_h_w),\n",
    "        tv_transfroms.Lambda(custom_transforms.one_hot),\n",
    "    ]\n",
    ")\n",
    "# some checks\n",
    "assert output_h_w == [520, 520] # this is the size for this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding this directory because the masks appear to be wrong\n",
    "EXCLUDE_DIRS = (Path('data/Hou/PV03_Ground_WaterSurface'),)\n",
    "hou_ds = make_hou_dataset(\n",
    "    EXCLUDE_DIRS,\n",
    "    img_transforms=input_transforms,\n",
    "    mask_transforms=mask_transforms,\n",
    ")\n",
    "\n",
    "# Split datasets\n",
    "train_ds, val_ds, test_ds = random_split(hou_ds, [0.6, 0.2, 0.2], generator=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kasmi ign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ign_ds = make_kasmi_ign_dataset(\n",
    "#     img_transforms=input_transforms,\n",
    "#     mask_transforms=mask_transforms,\n",
    "# )\n",
    "\n",
    "# # Split datasets\n",
    "# # Will test on the same dataset which was pickled last time\n",
    "# train_ds, val_ds = random_split(ign_ds, [0.8, 0.2], generator=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "train_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True, generator=rng)\n",
    "val_loader = DataLoader(val_ds, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss_mean = 0.\n",
    "\n",
    "    # Here, we use enumerate(train_loader) instead of\n",
    "    # iter(train_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        # move data to GPU\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)['out']\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimiser.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % N_BATCHES_FOR_RECORD == N_BATCHES_FOR_RECORD - 1:\n",
    "            last_loss_mean = running_loss / N_BATCHES_FOR_RECORD # loss per batch\n",
    "            print(f'\\tBatches {i-N_BATCHES_FOR_RECORD+2}-{i+1} mean loss: {last_loss_mean}')\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss_mean, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "\tBatches 1-10 mean loss: 0.47139993906021116\n",
      "\tBatches 11-20 mean loss: 0.302275624871254\n",
      "\tBatches 21-30 mean loss: 0.2746844723820686\n",
      "\tBatches 31-40 mean loss: 0.23926840275526046\n",
      "LOSS train 0.23926840275526046 valid 0.229450985789299\n",
      "EPOCH 2:\n",
      "\tBatches 1-10 mean loss: 0.20199502259492874\n",
      "\tBatches 11-20 mean loss: 0.216423699259758\n",
      "\tBatches 21-30 mean loss: 0.19614199846982955\n",
      "\tBatches 31-40 mean loss: 0.1848228767514229\n",
      "LOSS train 0.1848228767514229 valid 0.22434663772583008\n",
      "EPOCH 3:\n",
      "\tBatches 1-10 mean loss: 0.19168491363525392\n",
      "\tBatches 11-20 mean loss: 0.18057887256145477\n",
      "\tBatches 21-30 mean loss: 0.1842053860425949\n",
      "\tBatches 31-40 mean loss: 0.17903000712394715\n",
      "LOSS train 0.17903000712394715 valid 0.17176111042499542\n",
      "EPOCH 4:\n",
      "\tBatches 1-10 mean loss: 0.1735008955001831\n",
      "\tBatches 11-20 mean loss: 0.15665979757905008\n",
      "\tBatches 21-30 mean loss: 0.1480630874633789\n",
      "\tBatches 31-40 mean loss: 0.1437992848455906\n",
      "LOSS train 0.1437992848455906 valid 0.16064895689487457\n",
      "EPOCH 5:\n",
      "\tBatches 1-10 mean loss: 0.16798102259635925\n",
      "\tBatches 11-20 mean loss: 0.16745250895619393\n",
      "\tBatches 21-30 mean loss: 0.14703479856252671\n",
      "\tBatches 31-40 mean loss: 0.14390684068202972\n",
      "LOSS train 0.14390684068202972 valid 0.15561889111995697\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/ResNet-50{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            # move to GPU\n",
    "            vinputs = vinputs.to(device)\n",
    "            vlabels = vlabels.to(device)\n",
    "            voutputs = model(vinputs)['out']\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'saved_models/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_ds:\n",
    "    with open(f'saved_models/test_data/test_ds_{timestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(test_ds, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
